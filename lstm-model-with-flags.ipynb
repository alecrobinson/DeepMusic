{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alecr\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy\n",
    "from music21 import converter, instrument, note, chord, interval, pitch\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(transpose=False):\n",
    "    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
    "    print('starting get_notes')\n",
    "    notes = []\n",
    "\n",
    "    for file in glob.glob(\"C:/Users/alecr/Projects/temp-data/skuldur-classical-piano-composer/*.mid\"):\n",
    "        s = converter.parse(file)\n",
    "        \n",
    "        if (transpose):\n",
    "            k = s.analyze('key')\n",
    "            #print(k)\n",
    "            i = interval.Interval(k.tonic, pitch.Pitch('C'))\n",
    "            midi = s.transpose(i)\n",
    "            #print('piece was in ',k, 'now in ', midi.analyze('key'))\n",
    "        else:\n",
    "            midi = s\n",
    "        #print(i)\n",
    "        #print('s now in ', midi.analyze('key'))\n",
    "        #pieces.append(sNew)\n",
    "\n",
    "        #print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    #with open('data/notes', 'wb') as filepath:\n",
    "        #pickle.dump(notes, filepath)\n",
    "    print('done')\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting get_notes\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "notes = get_notes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumped\n"
     ]
    }
   ],
   "source": [
    "fileObject = open(\"C:/Users/alecr/Projects/deepmusic/pickledump/notes\",'wb')\n",
    "pickle.dump(notes,fileObject)\n",
    "fileObject.close()\n",
    "print('dumped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting that there isn't much of a difference in the size of the set of notes between transposed and not. A better metric would be to see a frequency distribution of the notes and compare those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 100\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "     # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "\n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#network_input = None\n",
    "#network_output = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non transposing original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting get_notes\n",
      "done\n",
      "notes and sequences prepared\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" Train a Neural Network to generate music \"\"\"\n",
    "notes = get_notes(transpose=False)\n",
    "\n",
    "# get amount of pitch names\n",
    "n_vocab = len(set(notes))\n",
    "\n",
    "network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "\n",
    "\n",
    "print('notes and sequences prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57077, 100, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57177"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        return_sequences=True\n",
    "))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"C:/Users/alecr/Projects/deepmusic/models/weights-improvement-60-1.5352-bigger.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1970\n",
      "Epoch 2/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1953\n",
      "Epoch 3/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1981\n",
      "Epoch 4/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1971\n",
      "Epoch 5/50\n",
      "57077/57077 [==============================] - 206s 4ms/step - loss: 0.1896\n",
      "Epoch 6/50\n",
      "57077/57077 [==============================] - 207s 4ms/step - loss: 0.1919\n",
      "Epoch 7/50\n",
      "57077/57077 [==============================] - 208s 4ms/step - loss: 0.1945\n",
      "Epoch 8/50\n",
      "57077/57077 [==============================] - 901s 16ms/step - loss: 0.1910\n",
      "Epoch 9/50\n",
      "57077/57077 [==============================] - 2340s 41ms/step - loss: 0.1948\n",
      "Epoch 10/50\n",
      "57077/57077 [==============================] - 3743s 66ms/step - loss: 0.1901\n",
      "Epoch 11/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1987\n",
      "Epoch 12/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1947\n",
      "Epoch 13/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1893\n",
      "Epoch 14/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1965\n",
      "Epoch 15/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1935\n",
      "Epoch 16/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1853\n",
      "Epoch 17/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1896\n",
      "Epoch 18/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1893\n",
      "Epoch 19/50\n",
      "57077/57077 [==============================] - 208s 4ms/step - loss: 0.1879\n",
      "Epoch 20/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1884\n",
      "Epoch 21/50\n",
      "57077/57077 [==============================] - 208s 4ms/step - loss: 0.1852\n",
      "Epoch 22/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1864\n",
      "Epoch 23/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1853\n",
      "Epoch 24/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1867\n",
      "Epoch 25/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1866\n",
      "Epoch 26/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1831\n",
      "Epoch 27/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1836\n",
      "Epoch 28/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1870\n",
      "Epoch 29/50\n",
      "57077/57077 [==============================] - 208s 4ms/step - loss: 0.1846\n",
      "Epoch 30/50\n",
      "57077/57077 [==============================] - 208s 4ms/step - loss: 0.1828\n",
      "Epoch 31/50\n",
      "57077/57077 [==============================] - 207s 4ms/step - loss: 0.1826\n",
      "Epoch 32/50\n",
      "57077/57077 [==============================] - 205s 4ms/step - loss: 0.1810\n",
      "Epoch 33/50\n",
      "57077/57077 [==============================] - 208s 4ms/step - loss: 0.1851\n",
      "Epoch 34/50\n",
      "57077/57077 [==============================] - 210s 4ms/step - loss: 0.1845\n",
      "Epoch 35/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1858\n",
      "Epoch 36/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1822\n",
      "Epoch 37/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1829\n",
      "Epoch 38/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1775\n",
      "Epoch 39/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1801\n",
      "Epoch 40/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1816\n",
      "Epoch 41/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1860\n",
      "Epoch 42/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1875\n",
      "Epoch 43/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1803\n",
      "Epoch 44/50\n",
      "57077/57077 [==============================] - 210s 4ms/step - loss: 0.1821\n",
      "Epoch 45/50\n",
      "57077/57077 [==============================] - 210s 4ms/step - loss: 0.1808\n",
      "Epoch 46/50\n",
      "57077/57077 [==============================] - 210s 4ms/step - loss: 0.1763\n",
      "Epoch 47/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1785\n",
      "Epoch 48/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1782\n",
      "Epoch 49/50\n",
      "57077/57077 [==============================] - 209s 4ms/step - loss: 0.1803\n",
      "Epoch 50/50\n",
      "57077/57077 [==============================] - 207s 4ms/step - loss: 0.1826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23cb4b522e8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"C:/Users/alecr/Projects/deepmusic/models/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "model.fit(network_input, network_output, epochs=50, batch_size=128, verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "250 epochs so far"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.save(\"C:/Users/alecr/Projects/deepmusic/models/lstm-original.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved weights\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"C:/Users/alecr/Projects/deepmusic/models/lstm-original-weights-2.hdf5\")\n",
    "print('saved weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transposer half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_t = \"C:/Users/alecr/Projects/deepmusic/models/weights-improvement-transposing{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath_t,\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "callbacks_list_t = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting get_notes\n",
      "done\n",
      "notes and sequences prepared\n"
     ]
    }
   ],
   "source": [
    "transposed_notes = get_notes(transpose=True)\n",
    "\n",
    "# get amount of pitch names\n",
    "n_vocab_t = len(set(transposed_notes))\n",
    "\n",
    "network_input_t, network_output_t = prepare_sequences(transposed_notes, n_vocab_t)\n",
    "\n",
    "\n",
    "print('notes and sequences prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle dump\n",
    "\n",
    "noteFileObject = open(\"C:/Users/alecr/Projects/deepmusic/pickledump/transposed_notes\",'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(transposed_notes,noteFileObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(network_input_t,fileObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(network_output_t,fileObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = Sequential()\n",
    "model_t.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(network_input_t.shape[1], network_input_t.shape[2]),\n",
    "        return_sequences=True\n",
    "))\n",
    "model_t.add(Dropout(0.3))\n",
    "model_t.add(LSTM(512, return_sequences=True))\n",
    "model_t.add(Dropout(0.3))\n",
    "model_t.add(LSTM(512))\n",
    "model_t.add(Dense(256))\n",
    "model_t.add(Dropout(0.3))\n",
    "model_t.add(Dense(n_vocab_t))\n",
    "model_t.add(Activation('softmax'))\n",
    "model_t.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t.load_weights(\"C:/Users/alecr/Projects/deepmusic/models//lstm-transposition-weights.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currently on training round 2\n",
    "60 + 60 epochs\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "57077/57077 [==============================] - 354s 6ms/step - loss: 4.7006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa8d002b00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_t.fit(network_input_t, network_output_t, epochs=1, batch_size=64 , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "model_t.save_weights(\"C:/Users/alecr/Projects/deepmusic/models//lstm-transposition-weights.hdf5\")\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57077, 100, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57077, 100, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57077, 358)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57077, 344)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_output_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'network_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-213600af5a1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnetwork_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'network_output' is not defined"
     ]
    }
   ],
   "source": [
    "network_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1051151653148060444\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6701754613\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7915996438733296818\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
